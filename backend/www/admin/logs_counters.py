# Copyright 2012 Viewfinder Inc. All Rights Reserved.

"""Handlers for viewing counters generated by logs analysis.
"""

__author__ = 'marc@emailscrubbed.com (Marc Berhault)'

import json
import os
import re
import time
import logging
from collections import Counter, defaultdict
from functools import partial
from tornado import auth, gen, template

from viewfinder.backend.base import constants, handler, counters
from viewfinder.backend.base.dotdict import DotDict
from viewfinder.backend.db import metric
from viewfinder.backend.db.db_client import RangeOperator
from viewfinder.backend.www.admin import admin

# Custom aggregation of metrics. Tuple of (regexp, sub_expr).
# If a metric name matches a regexp, we add its value to a metric whose name is determined by:
# new_metric = regexp.sub(sub_expr, original_name)
# We can match more than one regexp. Original metrics are then discarded.
#
# This works well if aggregated metrics have the same timestamp. If not, we will end up with more data points
# than expected (but summing across completely different metrics doesn't really make sense anyway).
#
# We use custom aggregate to rename some specific countries we're watching for, something to do
# with copyright or trademark something or other. Ask Harry.
# For the list of country codes, see viewfinder/backend/services/itunes_trends_codes.py
kSummedMetrics = [ (re.compile(r'itunes\.downloads.*'), r'itunes.downloads'),
                   (re.compile(r'itunes\.updates.*'), r'itunes.updates'),
                   (re.compile(r'itunes\.inapp_subscriptions_auto_renew.*'), r'itunes.inapp_subscriptions'),
                   (re.compile(r'itunes\.downloads\.\w+\.(AE|AR|BR|CH|CL|CO|IL|IN|IS|MX|NO|ZA)'),
                    r'watched_countries.downloads.\1'),
                   (re.compile(r'dynamodb.user.locale.([a-zA-Z]+)(?:(-|_)[a-zA-Z]+)?'), r'dynamodb.user.locale.\1'),
                 ]

# Custom metrics filter. This is applied after kSummedMetrics, so it is valid to put an aggregated metric here.
# Array of (regexp, allowed_groups). If a metric name matches the regexp, the extracted group must be in
# 'allowed groups'. If a regexp matches, it MUST extract exactly one item.
kFilteredMetrics = [ ('db\.table\.count\.(.*)', ['Comment', 'Episode', 'Photo', 'User', 'Viewpoint']),
                     ('db\.table\.size\.(.*)', []),
                   ]

class LogsCountersHandler(admin.AdminHandler):
  """Handler which returns the counter admin page."""
  @handler.authenticated()
  @handler.asynchronous(datastore=True)
  @admin.require_permission(level='support')
  def get(self):
    t_dict = self.PermissionsTemplateDict()
    t_dict['page_title'] = 'Viewfinder Logs Counters'
    t_dict['data_src'] = 'logs_counters_data'
    t_dict['default_interval'] = '14d'
    self.render("counters.html", **t_dict)


class LogsCountersDataHandler(admin.AdminHandler):
  """Handler which responds to ajax queries for performance counter data."""

  MAX_TICK_COUNT = 150
  """Desired maximum number of data points to return for a single query."""

  @handler.authenticated()
  @handler.asynchronous(datastore=True)
  @admin.require_permission(level='support')
  @gen.engine
  def get(self):
    """Responds to a single request for performance counter data.  Each request has two required
    parameters in the query string, 'start' and 'end', which specify the beginning and end of the
    time range to be queried.  The times should be expressed as the number of seconds since the
    unix epoch.
    """
    start_time = float(self.get_argument('start'))
    end_time = float(self.get_argument('end'))

    # Select an appropriate interval resolution based on the requested time span.
    selected_interval = metric.LOGS_INTERVALS[-1]
    group_key = metric.Metric.EncodeGroupKey(metric.LOGS_STATS_NAME, selected_interval)
    logging.info('Query performance counters %s, range: %s - %s, resolution: %s'
                  % (group_key, time.ctime(start_time), time.ctime(end_time), selected_interval.name))

    self.set_header('Content-Type', 'application/json; charset=UTF-8')

    metrics = list()
    start_key = None
    while True:
      new_metrics = yield gen.Task(metric.Metric.QueryTimespan, self._client, group_key,
                                   start_time, end_time, excl_start_key=start_key)
      if len(new_metrics) > 0:
        metrics.extend(new_metrics)
        start_key = metrics[-1].GetKey()
      else:
        break
    data = {'group_key': group_key, 'start_time': start_time, 'end_time': end_time}
    data['data'] = _SerializeMetrics(metrics)

    self.write(json.dumps(data))
    self.finish()

def _SerializeMetrics(metrics):
  def _SkipMetric(name):
    for regex, allowed_groups in kFilteredMetrics:
      res = re.match(regex, k)
      if res is None:
        continue
      assert len(res.groups()) == 1
      if res.groups()[0] in allowed_groups:
        return False
      else:
        return True
    return False

  def _AggregateMetric(running_sum, metric_name):
    """Given a metric name, determine whether we sum it into a different metric name or not.
    Returns whether the original metric needs to be processed.
    """
    keep = True
    for regex, replacement, in kSummedMetrics:
      res = regex.sub(replacement, metric_name)
      if res != metric_name:
        keep = False
        if not _SkipMetric(res):
          running_sum[res] += v
    return keep

  data = defaultdict(dict)
  prev_metrics = {}
  seen_vars = set()
  for m in metrics:
    running_sum = Counter()
    timestamp = m.timestamp
    payload = DotDict(json.loads(m.payload)).flatten()
    for k, v in payload.iteritems():
      keep_original = _AggregateMetric(running_sum, k)
      if keep_original and not _SkipMetric(k):
        running_sum[k] += v
    for k, v in running_sum.iteritems():
      d = data[k]
      if len(d) == 0:
        d['is_average'] = False
        d['cluster_total'] = list()
        d['cluster_rate'] = list()
        d['description'] = k
      d['cluster_total'].append((timestamp, v))
      if k in prev_metrics:
        _, prev_v = prev_metrics[k]
        # Since the metrics are written exactly once a day, no need to divide, just use the difference.
        diff = (v - prev_v)
      else:
        diff = v
      if k not in seen_vars:
        # Skip the first data point, we don't know what the previous value is.
        # We can't use prev_metrics since metrics with holes (eg: missing days) get removed.
        d['cluster_rate'].append((timestamp, None))
        seen_vars.add(k)
      else:
        d['cluster_rate'].append((timestamp, diff))
      prev_metrics[k] = (timestamp, v)
    # Look for metrics that haven't been set recently and insert None to break the graph.
    # Since we may have sets of metrics stored at various timestamps, we can't just do this at the next
    # time. Instead, we break the metric if we haven't seen a data point in slightly over one day.
    for k, (t, v) in prev_metrics.items():
      if (timestamp - t) > (constants.SECONDS_PER_DAY + constants.SECONDS_PER_HOUR):
        # data[k] can't be empty since we've seen this key before.
        data[k]['cluster_total'].append((timestamp, None))
        data[k]['cluster_rate'].append((timestamp, -v))
        # Remove it so we don't send back lots of data for no reason.
        del prev_metrics[k]

  return data
